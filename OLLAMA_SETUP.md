# ü¶ô Ollama Be√°ll√≠t√°s (Lok√°lis, Teljesen Ingyenes)

Ollama haszn√°lata a History Tutor AI-ban - teljesen ingyenes, lok√°lis AI modell futtat√°s.

## üì¶ Telep√≠t√©s

### 1. Ollama telep√≠t√©se

**Windows:**
1. T√∂ltsd le: https://ollama.com/download/windows
2. Futtasd a telep√≠t≈ët
3. Az Ollama automatikusan elindul a h√°tt√©rben

**Mac:**
```bash
brew install ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### 2. Modell let√∂lt√©se

Nyisd meg a termin√°lt √©s futtasd:

```bash
ollama pull llama3.2
```

Vagy m√°s modellt is haszn√°lhatsz:
- `llama3.2` - Kisebb, gyorsabb (aj√°nlott kezd√©shez)
- `llama3.1` - Nagyobb, pontosabb
- `mistral` - Alternat√≠va
- `phi3` - Microsoft modell

### 3. Ellen≈ërz√©s

Ellen≈ërizd, hogy az Ollama fut:

```bash
ollama list
```

Ez meg kell mutassa a let√∂lt√∂tt modelleket.

### 4. K√∂rnyezeti v√°ltoz√≥k (opcion√°lis)

A `.env.local` f√°jlban be√°ll√≠thatod (de alap√©rtelmezetten m≈±k√∂dik):

```env
USE_OLLAMA=true
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
```

**Megjegyz√©s:** Ha nem √°ll√≠tod be, az Ollama alap√©rtelmezett lesz!

## üöÄ Haszn√°lat

1. Ind√≠tsd el az Ollama-t (√°ltal√°ban automatikusan fut)
2. Ind√≠tsd el a Next.js dev szervert:
   ```bash
   npm run dev
   ```
3. K√©sz! Az alkalmaz√°s automatikusan az Ollama-t fogja haszn√°lni.

## üîß Hibaelh√°r√≠t√°s

### "Cannot connect to Ollama"

1. **Ellen≈ërizd, hogy az Ollama fut:**
   ```bash
   ollama list
   ```
   Ha hiba√ºzenetet kapsz, ind√≠tsd el az Ollama-t.

2. **Windows-on:**
   - Nyisd meg a Task Manager-t
   - N√©zd meg, hogy fut-e az "Ollama" folyamat
   - Ha nem, ind√≠tsd el az Ollama alkalmaz√°st

3. **Port ellen≈ërz√©s:**
   - Nyisd meg: http://localhost:11434
   - Ha nem t√∂lt be, az Ollama nem fut

### "Model not found"

Futtasd:
```bash
ollama pull llama3.2
```

### Lass√∫ v√°laszid≈ë

- A kisebb modellek (pl. `llama3.2`) gyorsabbak
- A nagyobb modellek pontosabbak, de lassabbak
- Els≈ë futtat√°skor lassabb lehet (modell bet√∂lt√©se)

## üìä El√©rhet≈ë modellek

N√©zd meg az √∂sszes el√©rhet≈ë modellt:
```bash
ollama list
```

N√©pszer≈± modellek:
- `llama3.2` - 3B param√©ter, gyors, j√≥ min≈ës√©g
- `llama3.1:8b` - 8B param√©ter, jobb min≈ës√©g
- `mistral` - Alternat√≠va, j√≥ teljes√≠tm√©ny
- `phi3` - Microsoft, kisebb modell

## üí° Tippek

1. **Els≈ë haszn√°lat:** A `llama3.2` modell j√≥ v√°laszt√°s - gyors √©s j√≥ min≈ës√©g≈±
2. **Jobb min≈ës√©g:** Ha t√∂bb RAM-od van, pr√≥b√°ld ki a `llama3.1:8b` modellt
3. **Offline haszn√°lat:** Az Ollama teljesen offline m≈±k√∂dik, nincs sz√ºks√©g internetre
4. **Teljes√≠tm√©ny:** A GPU haszn√°lata jelent≈ësen felgyors√≠tja a v√°laszid≈ët

## üéØ El≈ëny√∂k

‚úÖ **Teljesen ingyenes** - nincs API kulcs, nincs k√∂lts√©g  
‚úÖ **Lok√°lis** - az adataid nem mennek fel a felh≈ëbe  
‚úÖ **Offline** - m≈±k√∂dik internet n√©lk√ºl  
‚úÖ **Priv√°t** - teljes adatv√©delem  
‚úÖ **Korl√°tlan** - nincs rate limit vagy quota  

## ‚ö†Ô∏è H√°tr√°nyok

- Sz√ºks√©ges helyi telep√≠t√©s
- RAM ig√©nyes (legal√°bb 8GB aj√°nlott)
- Els≈ë futtat√°skor lassabb lehet

